#!/usr/bin/env bash

# Copyright (c) 2013
# Harvard FAS Research Computing
# John Brunelle <john_brunelle@harvard.edu>
# All right reserved.

set -e

helpstr="\
NAME
    hpc-hadoop-mapreduce - run hadoop mapreduce on HPC clusters

SYNOPSIS
    hpc-hadoop-mapreduce [OPTIONS] -- hadoop jar JARFILE ...

DESCRIPTION
    There are two main purposes for this script:

        (1) to allow multiple users to run multiple hadoop mapreduce jobs 
            concurrently, independently, and under the provisions of the 
            cluster batch job system

        (2) to allow direct i/o to shared network storage -- no HDFS

    Normally one uses hadoop to manage distributed storage and multiple jobs 
    using that storage.  Here, these features are not used.  Instead, one uses 
    this script to submit a hadoop mapreduce computation as a plain, parallel 
    HPC cluster batch job, and this script starts up its own, independent, 
    transient hadoop JobTracker and TaskTracker daemons based upon the compute 
    resources allocated to it by the batch job system.  Hadoop resources, such 
    as network ports and temporary files, are configured to be isolated from 
    other such batch jobs.

    Furthermore, no Hadoop Distributed File System is used.  The transient 
    mapreduce framework this script builds is configured to do i/o directly 
    with cluster storage (the current working directory by default, or 
    --data-dir if given).  This is designed for the case when data resides on a 
    high-performance, parallel filesystem and one wants to skip the data 
    stage-in and stage-out steps done in a conventional hadoop workflow.

    The batch job integration currently works with SLURM and LSF and is 
    determined by the presence of the associated environment variables (SLURM 
    is checked for first).

    See the \`EXAMPLE' section below for specific usage directions.

DETAILS
    This script needs to know where the hadoop and java configuration and 
    software are installed.  This can be accomplished by setting environment 
    variables or using command line options (see below for details).

    This script needs to know what filesystem path to use for storage.  This 
    should be a path to shared, network storage containing all job inputs and 
    where all job outputs are written.  A directory named \`hadoop' is also put 
    in the root of this location and is used to hold hadoop's shared state and 
    logs.  By default the current working directory is used; there is the 
    command line option --data-dir available to use something else.

    Then one just runs hadoop jar etc., prefixed by this script:

        $ hpc-hadoop-mapreduce -- hadoop jar JARFILE [OPTIONS]

    If one runs this as a batch job, one should use options to set an even 
    distribution of job slots per host.  For SLURM, use something like -n X for 
    the number of job slots and --ntasks-per-node=Y for the distribution.  For 
    LSF, use something like -n X and -R 'span[ptile=Y]'.

    Under the hood, this script copies the default \`conf' directory from the 
    hadoop software installation to the current working directory (which may or 
    may not be the same as the shared storage) and tweaks the configuration so 
    that each job uses shared storage and runs independently from other jobs.  
    Specifically, it:

        * Sets the required JAVA_HOME.

        * Sets fs.default.name to the shared storage (using file:// instead of 
          hdfs://).

        * Sets mapred.system.dir and mapreduce.jobtracker.staging.root.dir, 
          which need to be in a location accessible by all hosts, to 
          \`hadoop/mapred/system' and \`hadoop/mapred/staging', respectively, 
          within the shared storage.  (By default, when hdfs is not used, 
          hadoop uses locations under hadoop.tmp.dir, which are not shared, and 
          jobs fail.)

        * Sets HADOOP_LOG_DIR to use \`hadoop/logs' within the shared storage.  
          (By default hadoop writes to the software installation area).

        * Finds free ports on which to run the JobTracker and the JobTracker 
          and TaskTracker http interfaces (mapred.job.tracker, 
          mapred.job.tracker.http.address, and 
          mapred.task.tracker.http.address, respectively).  One must view 
          stdout to see what it picks.

        * Sets hadoop.tmp.dir to use /tmp/hadoop-\${USER}-\${jobidstr} where 
          \$jobidstr is a unique identifier (the batch job id or a combination 
          of time/host/pid).  This keeps each job in its own space rather than 
          having all under the default /tmp/hadoop-\${USER}.

        * Sets HADOOP_PID_DIR to use hadoop.tmp.dir.  (By default hadoop uses 
          /tmp, which does not allow for independent trackers to run 
          simultaneously.)

    It also decides where and how to run the trackers.  The script takes 
    different action based upon the presence or absence of specific batch job 
    system environment variables.  If \$SLURM_JOB_NODELIST, the variable SLURM 
    uses to list the hosts allocated to the job, or \$LSB_HOSTS, the variable 
    LSF uses, is non-empty, this:

        * Sets slaves to be the unique hosts listed.

        * Sets the maximum tasks per host (mapred.tasktracker.map.tasks.maximum 
          and mapred.tasktracker.reduce.tasks.maximum).

          For SLURM, this is taken from the environment variable 
          \$SLURM_NTASKS_PER_NODE.  If the variable is not present, the maximum 
          tasks per host is set to 1.

          For LSF, this is set to the number of appearances of the 
          least-repeated host in \$LSB_HOSTS.

          Thus, for unbalanced job slot allocations, this errs on the side of 
          under-utilization so that other batch jobs are not disturbed.  
          Instead, one should always specify a ntasks-per-node or ptile when 
          submitting the batch job so that allocations are even and fully used.

          Note that all hosts run the same number of TaskTrackers, including 
          the master host that is running this script and therefore also 
          running the JobTracker.

    Otherwise, the job runs on the localhost with maximum map and reduce tasks 
    set to 1.

    This script then starts the mapreduce deamons, runs the jobs, stops the 
    daemons, and removes the temporary directories.

OPTIONS
    --data-dir PATH
        The absolute path to the root of the storage used for the job.  This 
        includes job inputs and outputs, and a directory named \`hadoop' will 
        be created here (if it does not already exist) to hold hadoop's shared 
        state and logs.

        The default is the current working directory.

    --hadoop-dir PATH
        The path to the root of the hadoop installation to use.  If not given, 
        this will use \$HADOOP_HOME, if present in the environment.  
        (\$HADOOP_HOME, for hadoop's purposes, is actually deprecated, so this 
        unsets it before running the job.)

        Note that the bin subdirectory of this directory is added to the PATH.

        This can be a read-only location (unlike many standard hadoop 
        examples).

    --java-dir PATH
        The absolute path to the root of the java installation to use.  If not 
        given, this will use \$JAVA_HOME, if present in the environment.

    --force
        Overwrite existing things if present (currently this only applies to 
        the conf directory created in the current working directory).

    -h, --help
        Print this help.

REQUIREMENTS
    Hadoop.  This is designed to work with version 1.0.4 and may or may not 
    work well with other versions.

    Java.  As required by Hadoop.

EXAMPLE
    The following runs the grep example that ships with hadoop on the Harvard 
    FAS RC Odyssey cluster.  This is the mapreduce version of the following 
    unix pipeline:

        $ find input -type f | \\
          xargs grep -oPh 'a|the' | \\
          sort | uniq -c | sort -nr

    where input is a directory containing text files.

    * CD to an empty scratch directory on cluster storage (e.g. your home 
      directory).

    * Copy in some sample input text files (these are just used a source of 
      text, the fact that they are from hadoop does not matter):

        $ mkdir -p input
        $ cp -a /n/sw/hadoop-1.0.4/conf/*.xml input/

    * Create a job definition file named job.batch with the following contents 
      (if using LSF, use \"-R 'span[ptile=2]'\" instead of 
      --ntasks-per-node=2, among other things):

        #!/usr/bin/env bash

        #SBATCH -p general
        #SBATCH -t 5
        #SBATCH --mem 300
        #SBATCH -o job.out
        #SBATCH -e job.err
        #SBATCH -n 6
        #SBATCH --ntasks-per-node=2

        export HADOOP_HOME=/n/sw/hadoop-1.0.4
        export JAVA_HOME=/n/sw/jdk1.6.0_30

        hpc-hadoop-mapreduce \\
          hadoop jar \"\$HADOOP_HOME\"/hadoop-examples-*.jar \\
          grep input output 'a|the'

    * Load the module that provides this script:

        $ module load hpc/rc

    * Submit the job (if using LSF, use \"bsub < job.batch\"):

        $ sbatch job.batch

    * Wait for the job to finish.

    * Check the job output:

        $ cat output/part-00000

      It should give the following word counts:

          841     a
          92      the

      which should match the output of the shell pipeline above.

BUGS/TODO
    * port binding race condition

      Free ports are found by trial-and-error, and there is a window of time 
      between choosing the port to use and when trackers start up. It is 
      possible the port will taken by something else in the meantime and the 
      job will fail.

    * cleanup

      The goal is for all cleanup to be automatic, but what happens when the 
      batch job master kills the job has not yet been tested.  It is not known 
      if there is reasonable time for the exit handlers to complete before the 
      batch job master forcefully kills the job.  A post-run script may end up 
      being necessary.

AUTHOR
    Copyright (c) 2013
    Harvard FAS Research Computing
    John Brunelle <john_brunelle@harvard.edu>
    All right reserved.
"

function log() {
	echo "$(date '+%Y-%m-%d_%H:%M:%S.%N'): $(basename "$0"): $1"
}

hadoop_dir=''
data_dir=''
java_dir=''

force=false

args=$(getopt -l hadoop-dir:,data-dir:,java-dir:,force,help -o h -- "$@")
if [ $? -ne 0 ]; then
	exit 65  #(getopt will have written the error message)
fi
eval set -- "$args"
while [ ! -z "$1" ]; do
	case "$1" in
		--hadoop-dir)
			hadoop_dir="$2"
			shift
			;;
		--data-dir)
			data_dir="$2"
			shift
			;;
		--java-dir)
			java_dir="$2"
			shift
			;;

		--force)
			force=true
			;;

		-h | --help)
			echo -n "$helpstr"
			exit 0
			;;
		--) 
			shift
			break
			;;
	esac
	shift
done

#jobidstr is meant to be a cluster-wide unique identifer
if [ -n "$SLURM_JOB_ID" ]; then
	jobidstr="$SLURM_JOB_ID"
elif [ -n "$LSB_JOBID" ]; then
	jobidstr="$LSB_JOBID"
else
	jobidstr="$(date '+%Y-%m-%d_%H:%M:%S')_$(hostname -s)_"$$
fi

if [ -z "$data_dir" ]; then
	#echo "*** ERROR *** --data-dir not given" >&2
	#exit 1
	data_dir="$PWD"
fi
log "using data directory: $data_dir"

if [ -z "$hadoop_dir" ]; then
	if [ -n "$HADOOP_HOME" ]; then
		hadoop_dir="$HADOOP_HOME"
	else
		echo "*** ERROR *** --hadoop-dir not given and \$HADOOP_HOME not set" >&2
		exit 1
	fi
fi
log "using hadoop directory: $hadoop_dir"

if [ -z "$java_dir" ]; then
	if [ -n "$JAVA_HOME" ]; then
		java_dir="$JAVA_HOME"
	else
		echo "*** ERROR *** --java-dir not given and \$JAVA_HOME not set" >&2
		exit 1
	fi
fi
log "using java directory: $java_dir"

hadoop_tmp_dir="/tmp/hadoop-$USER-$jobidstr"
if echo "$hadoop_tmp_dir" | grep -q "'"; then
	echo "*** ERROR *** hadoop_tmp_dir cannot contain single quotes" >&2
	exit 1
fi
log "using temporary directory: $hadoop_tmp_dir"

export PATH="$hadoop_dir/bin:$PATH"

set -u


#---


function free_port() {
	#usage: free_port [min [max]]
	#returns status 1 if it can't find one
	#does not verify min and max, if given, fall within ip_local_port_range

	pmin=${1:-$(cat /proc/sys/net/ipv4/ip_local_port_range | awk '{print $1}')}
	pmax=${2:-$(cat /proc/sys/net/ipv4/ip_local_port_range | awk '{print $2}')}

	set +e
	gotone=false
	for (( p=$pmin; p<=$pmax; p++ )); do
		#log "trying port $p"
		nc -l $p </dev/zero &>/dev/null &
		sleep 0.5  #give netcat some time to bind
		echo xquit | telnet -ex localhost $p &>/dev/null
		[ $? -eq 0 ] && gotone=true
		jobs -p | xargs -r kill &>/dev/null  #if nc can bind but something is blocking localhost telnet, then the nc needs to be killed
		$gotone && break
	done
	set -e

	if [ "$p" -gt "$pmax" ]; then
		#echo "*** ERROR *** unable to find a free port between $pmin and $pmax" >&2
		return 1
	fi

	echo "$p"
}


#--- find some free ports

#note the available port range
read pmin pmax < /proc/sys/net/ipv4/ip_local_port_range

#hash this job's unique id to get a 32-bit number (8 hex digits)
hash32=$(echo "$jobidstr" | md5sum | awk '{print substr($1,0,8)}')

#use the hash to pick a starting point in the bottom half of the available range
pmin=$(( $pmin + 0x$hash32 % (($pmax - $pmin)/2) ))

#JobTracker
log "search for a free port for the JobTracker to use, starting at port $pmin"
if ! job_tracker_port="$(free_port "$pmin" "$pmax")"; then
	echo "*** ERROR *** unable to find a free port between $pmin and $pmax" >&2
	exit 1
fi
log "using JobTracker port $job_tracker_port"
pmin=$(( $job_tracker_port + 1 ))

#JobTracker http
log "search for a free port for the JobTracker http to use, starting at port $pmin"
if ! job_tracker_http_port="$(free_port "$pmin" "$pmax")"; then
	echo "*** ERROR *** unable to find a free port between $pmin and $pmax" >&2
	exit 1
fi
log "using JobTracker http port $job_tracker_http_port"
pmin=$(( $job_tracker_http_port + 1 ))

#TaskTracker http
log "search for a free port for the TaskTracker http to use, starting at port $pmin"
if ! task_tracker_http_port="$(free_port "$pmin" "$pmax")"; then
	echo "*** ERROR *** unable to find a free port between $pmin and $pmax" >&2
	exit 1
fi
log "using TaskTracker http port $task_tracker_http_port"
pmin=$(( $task_tracker_http_port + 1 ))


#--- config

log "creating conf directory for this job: $PWD/conf"

#be conservative and don't overwrite existing files
if [ -d conf ] && ! $force; then
	echo "*** ERROR *** directory \`conf' already exists; remove it or use --force to overwrite it" >&2
	exit 1
fi

#start with default config
rsync -a --delete "$hadoop_dir"/conf/ conf/

#set JAVA_HOME
sed -i 's?# export JAVA_HOME=/usr/lib/j2sdk1.5-sun?JAVA_HOME='"$java_dir"'?' conf/hadoop-env.sh

#set HADOOP_LOG_DIR
sed -i 's?# export HADOOP_LOG_DIR=${HADOOP_HOME}/logs?export HADOOP_LOG_DIR='"$data_dir"'/hadoop/logs?' conf/hadoop-env.sh

#set HADOOP_PID_DIR (the default is actually just /tmp, not what's commented in the config)
sed -i 's?# export HADOOP_PID_DIR=/var/hadoop/pids?export HADOOP_PID_DIR='"$hadoop_tmp_dir"'?' conf/hadoop-env.sh

#set slaves
if [ -n "${SLURM_JOB_NODELIST:-}" ]; then
	scontrol show hostname "$SLURM_JOB_NODELIST" | sort | uniq > conf/slaves  #(uniq is not be necessary, it's here just in case things change)
elif [ -n "${LSB_HOSTS:-}" ]; then
	echo "$LSB_HOSTS" | tr ' ' '\n' | sort | uniq > conf/slaves
else
	echo "localhost" > conf/slaves
fi
log "using hosts $(cat conf/slaves | tr '\n' ',' | sed 's/,$//')"

#set max parallel processes per host
if [ -n "${SLURM_NTASKS_PER_NODE:-}" ]; then
	max_tasks_per_host="$SLURM_NTASKS_PER_NODE"
elif [ -n "${LSB_HOSTS:-}" ]; then
	max_tasks_per_host="$(echo "$LSB_HOSTS" | tr ' ' '\n' | sort | uniq -c | awk '{print $1}' | sort -n | head -n 1)"
else
	max_tasks_per_host=1
fi
log "using max_tasks_per_host $max_tasks_per_host"

#core config
cat > conf/core-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
		<name>hadoop.tmp.dir</name>
		<value>$hadoop_tmp_dir</value>
	</property>
	<property>
		<name>fs.default.name</name>
		<value>file://$data_dir</value>
	</property>
</configuration>
EOF

#mapred config
cat > conf/mapred-site.xml <<EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
		<name>mapred.job.tracker</name>
		<value>$(hostname -s):$job_tracker_port</value>
	</property>
	<property>
		<name>mapred.job.tracker.http.address</name>
		<value>0.0.0.0:$job_tracker_http_port</value>
	</property>
	<property>
		<name>mapred.task.tracker.http.address</name>
		<value>0.0.0.0:$task_tracker_http_port</value>
	</property>

	<property>
		<name>mapred.system.dir</name>
		<value>\${fs.default.name}/hadoop/mapred/system</value>
	</property>
	<property>
		<name>mapreduce.jobtracker.staging.root.dir</name>
		<value>\${fs.default.name}/hadoop/mapred/staging</value>
	</property>

	<property>
		<name>mapred.tasktracker.map.tasks.maximum</name>
		<value>$max_tasks_per_host</value>
	</property>
	<property>
		<name>mapred.tasktracker.reduce.tasks.maximum</name>
		<value>$max_tasks_per_host</value>
	</property>
</configuration>
EOF


#--- check that ssh works

while read h; do
	log "checking that prompt-less, password-less ssh works to host $h"  #(this doesn't check the full graph)
	if ! ssh -o PreferredAuthentications=gssapi-with-mic,hostbased,publickey -o StrictHostKeyChecking=yes "$h" true </dev/null &>/dev/null; then
		echo "*** ERROR *** unable to ssh cleanly to host $h"
		exit 1
	fi
done < conf/slaves


#--- set atexit cleanup

function cleanup() {
	set +e
	log "stopping the hadoop daemons"
	stop-mapred.sh

	if ! [[ "$hadoop_tmp_dir" = /tmp/* ]]; then
		log "hadoop_tmp_dir $hadoop_tmp_dir is not in /tmp, skipping removal, out of paranoia"
	else
		while read h; do
			log "removing the temporary directory on host $h"
			if ! ssh "$h" "rm -fr '$hadoop_tmp_dir'" </dev/null; then
				echo "*** WARNING *** unable to remove $hadoop_tmp_dir on host $h"
			fi
		done < conf/slaves
	fi
	set -e
}

trap cleanup EXIT


#--- start the daemons

log "starting the hadoop daemons"

export HADOOP_CONF_DIR="$PWD/conf"
unset HADOOP_HOME  ##hadoop complains that it's deprecated
start-mapred.sh


#---

log "running the mapreduce job"

"$@"
